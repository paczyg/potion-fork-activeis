[DEFAULT]
path                = results_cartpole_chi2
repetitions         = 10
iterations          = 1

# Environment
env                 = cartpole
horizon             = 200

# Policy
mu_init             = 0.0
logstd_init         = 0.0
learn_std           = False
action_filter       = None

# Common algorithm parameters
baseline            = 'zero'
estimator           = 'gpomdp'
seed                = 42

# Offpolicy algorithm parameters
divergence          = 'chi2'
ce_batchsizes       = '[50]'
batchsize           = 50

[means]
experiment = grid
mu_init = [-1.0, -0.5, 0.0, 0.5, 1.0]

[stds]
experiment = grid
logstd_init = [-1.0, -0.5, 0.0, 0.5, 1.0]