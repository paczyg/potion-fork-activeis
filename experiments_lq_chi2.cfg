[DEFAULT]
path                = results_lq_chi2
repetitions         = 30
iterations          = 1

# Environment
env                 = lq
state_dim           = 1
horizon             = 10

# Policy
mu_init             = 0.0
logstd_init         = 0.0
learn_std           = False
action_filter       = None

# Common algorithm parameters
baseline            = 'zero'
estimator           = 'gpomdp'
seed                = 42

# Offpolicy algorithm parameters
divergence          = 'chi2'
ce_batchsizes       = '[20]'
batchsize           = 20

[means]
experiment = grid
mu_init = [-1.0, -0.5, 0.0, 0.5, 1.0]

[stds]
experiment = grid
logstd_init = [-1.0, -0.5, 0.0, 0.5, 1.0]

[horizons]
experiment = grid
horizon = [1,2,5,10]

[dimensions]
experiment = grid
state_dim = [2, 5, 10]

[debug]
mu_init = [1.0]