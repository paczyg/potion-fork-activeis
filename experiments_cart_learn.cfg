[DEFAULT]
path = cartpole/results/paper
repetitions = 5
iterations = 200

# Policy
mu_init = 0.0
logstd_init = 0.0
learn_std = False

# Environment
environment = 'cartpole'
sigma_noise = 0
horizon = 500

# Common algorithm parameters
baseline = 'peters'
estimator = 'gpomdp'
seed = 42
stepper = 'ConstantStepper(1e-2)'

# Offpolicy algorithm parameters
biased_offpolicy = False
ce_batchsizes = [[20], [20], [20], [20], [20]]
ce_use_offline_data = False
ce_initialize_behavioural_policies = 'target'
ce_tol_grad = 1e3
ce_lr = 1e-4
ce_max_iter = 1e3
ce_mis_normalize = False
ce_mis_clip = None
ce_optimizer = 'adam'
ce_weight_decay = 10
ce_optimize_mean = True
ce_optimize_variance = True
#defensive_batch = 0
#njt_batchsize = 10

#[onpolicy]
#experiment = list
#batchsize = [100]
#ce_batchsizes = None

[offpolicy]
experiment = list
batchsize = [80, 66,53, 40, 27]
defensive_batch = [0, 14, 27, 40, 53]